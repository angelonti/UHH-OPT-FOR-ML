{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc565ac7-a7cb-43aa-8e7b-7cd17901667f",
   "metadata": {},
   "source": [
    "# Exercise sheet 8\n",
    "\n",
    "**Please turn in your exercises by January 16th.**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Angel Ontiveros, Björn Plüster\n",
    "## 16.01.2024"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad224ba1ecac6f21"
  },
  {
   "cell_type": "markdown",
   "id": "1fa27df4-a594-4f98-abea-c78454c2780e",
   "metadata": {},
   "source": [
    "## Projected gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e871567-aaa4-4bc9-809f-92536d8a061c",
   "metadata": {},
   "source": [
    "### Task 1: Standard simplex\n",
    "\n",
    "We considered projections onto the standard simplex in the lecture. Prove Lemma 10.4, i.e.,\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "Under the assumption that $v_{1} \\geq v_{2} \\geq \\cdots \\geq v_{d}$, with $x^{*}(p)$ as\n",
    "$$\n",
    "x^{*}(p):=(v_{1}-\\Theta_{p}, \\ldots, v_{p}-\\Theta_{p}, 0, \\ldots, 0), \\quad p \\in\\{1, \\ldots, d\\},\n",
    "$$\n",
    "and with\n",
    "$$\n",
    "p^{*}:=\\max \\left\\{p \\in\\{1, \\ldots, d\\}: v_{p}-\\frac{1}{p}\\left(\\sum_{i=1}^{p} v_{i}-1\\right)>0\\right\\},\n",
    "$$\n",
    "it holds that\n",
    "$$\n",
    "\\underset{x \\in \\Delta_{d}}{\\text{argmin }}\\|{x-v}\\|^{2}=x^{*}(p^{*}) .\n",
    "$$\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "From the lecture we have that\n",
    "$$\n",
    "\\Theta_{p} = \\frac{1}{p}\\left(\\sum_{i=1}^{p} v_{i}-1\\right),\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "p^{*} := \\max \\left\\{p \\in\\{1, \\ldots, d\\}: v_{p}-\\Theta_{p}>0\\right\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "For $x^*(p^*)$ to be in the standard simplex $\\Delta_{d}$, it must satisfy two conditions:\n",
    "- Non-negativity: $x_i \\geq 0$ for all $i$.\n",
    "- Summation to 1: $\\sum_{i=1}^d x_i = 1$.\n",
    "\n",
    "$x^*(p^*)$ satisfies the non-negativity condition since for $i \\leq p^*$, $x_i = v_i - \\Theta_{p^*} \\geq 0$ (by definition of $p^*$), and for $i > p^*$, $x_i = 0$.\n",
    "\n",
    "For any $p$ the summation equals 1:\n",
    "$$\n",
    "\\sum_{i=1}^{d} x_i^*(p) = \\sum_{i=1}^{p} (v_i - \\Theta_p) = \\sum_{i=1}^{p} v_i - p \\cdot \\Theta_p = \\sum_{i=1}^{p} v_i - \\sum_{i=1}^{p} v_i + 1 = 1.\n",
    "$$\n",
    "\n",
    "\n",
    "We need to show that for any $x \\in \\Delta_d$, $\\|x^*(p^*) - v\\|^2 \\leq \\|x - v\\|^2$.\n",
    "\n",
    "Let's consider an arbitrary $x \\in \\Delta_d$. The squared Euclidean distance to $v$ is given by:\n",
    "$$\n",
    "\\|x - v\\|^2 = \\sum_{i=1}^{d} (x_i - v_i)^2.\n",
    "$$\n",
    "\n",
    "If we choose a larger $p$ than $p^*$ we would violate the simplex constraint, since the values of x^*(p) would be negative for $i > p^*$.\n",
    "On the other hand, if we choose a smaller $p$ than $p^*$, we would have $x_i = 0$ for $i > p$, and would have contributions to the distance from $(v_i)^2$ that are bigger,\n",
    "since the components of $v$ are non-increasing. This $p^*$ is the largest $p$ that satisfies the simplex constraint, and minimizes the distance to $v$. $\\square$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3173a-453d-48fa-b0c0-8fddb861481c",
   "metadata": {},
   "source": [
    "### Task 2: Probability\n",
    "\n",
    "Suppose we are given a regression problem where we want to estimate a discrete probability distribution. It can be written in the following form:\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\displaystyle\\min_{w\\in\\mathbb{R}^d} & \\left\\|{Xw-y}\\right\\|_2^2\\\\\n",
    "\\text{s.t.}& \\|{w}\\|_1 = 1 \\\\\n",
    "& w \\geq 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The problem can be solved using projected gradient descent. Implement such an algorithm for solving this problem. Run it on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eba333e-a1d4-44a9-9f3f-50264bd86c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T17:55:22.643621100Z",
     "start_time": "2024-01-16T17:55:22.636495400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X)\n",
    "\n",
    "def g(w):\n",
    "    return 2 * X.T @ (X @ w - y) / len(X)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "y /= 1000\n",
    "X /= 10\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "\n",
    "def projected_gradient_descent(w, lr, threshold, max_iter):\n",
    "    iter = 0\n",
    "    old_grad_norm = float('inf')\n",
    "    while True:\n",
    "        grad = g(w)\n",
    "        w -= lr * grad\n",
    "        w = np.maximum(w, 0)\n",
    "        w /= np.sum(w)\n",
    "        iter += 1\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        grad_norm_diff = grad_norm - old_grad_norm\n",
    "        if grad_norm_diff < threshold or iter > max_iter:\n",
    "            break\n",
    "        old_grad_norm = grad_norm\n",
    "    return w, iter, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num iterations: 1\n",
      "gradient norm diff: 0.07684944367859972\n",
      "[0.02062346 0.01327478 0.00249736 0.0102525  0.00056743 0.00246996\n",
      " 0.         0.04321136 0.00393971 0.00779603 0.0358919  0.00438596\n",
      " 0.00490427 0.00656058 0.         0.00825194 0.         0.00764423\n",
      " 0.         0.04142642 0.00289623 0.01430778 0.         0.00703371\n",
      " 0.         0.00080746 0.         0.0067681  0.04018039 0.04018011\n",
      " 0.         0.         0.00459755 0.00659429 0.00841634 0.01793307\n",
      " 0.         0.         0.00790067 0.00081205 0.00522957 0.\n",
      " 0.         0.         0.         0.00955695 0.02050079 0.\n",
      " 0.02258699 0.00496016 0.001968   0.01617429 0.00837852 0.00302537\n",
      " 0.02794392 0.         0.03420394 0.         0.00406956 0.04113044\n",
      " 0.04224314 0.01428942 0.         0.00152479 0.00116339 0.01704434\n",
      " 0.0072764  0.00155718 0.00665287 0.01634742 0.         0.01309678\n",
      " 0.00575232 0.         0.00012424 0.04679701 0.00166455 0.00967855\n",
      " 0.         0.01199785 0.         0.         0.         0.03448475\n",
      " 0.01110778 0.00421575 0.01600318 0.0069338  0.         0.\n",
      " 0.0013083  0.0266545  0.01557153 0.03092192 0.00610596 0.00633705\n",
      " 0.         0.03472088 0.01605785 0.02051233]\n"
     ]
    }
   ],
   "source": [
    "lr = 1\n",
    "threshold = 1e-6\n",
    "\n",
    "w = np.zeros(X.shape[1])\n",
    "max_iter = 1000\n",
    " \n",
    "w, iter, grad_norm_diff = projected_gradient_descent(w, lr, threshold, max_iter)\n",
    "print(f'num iterations: {iter}')\n",
    "print(f'gradient norm diff: {grad_norm_diff}')\n",
    "print(w)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T17:55:44.274297300Z",
     "start_time": "2024-01-16T17:55:44.267188900Z"
    }
   },
   "id": "3c74642df84838c6",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "93e02960-7338-4aa6-90c2-886b15f2e42f",
   "metadata": {},
   "source": [
    "## Proximal gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379adcd-a155-4a5f-8dff-97054e16a91c",
   "metadata": {},
   "source": [
    "### Task 3: Proximal operator\n",
    "\n",
    "The LASSO is the following regularized optimization problem:\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\displaystyle\\min_{w\\in\\mathbb{R}^d} & \\frac{1}{n} \\left\\|{Xw-y}\\right\\|_2^2 + \\lambda \\|{w}\\|_1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Write down the formula of the proximal operator for this case, and prove that the subgradient of the proximal operator contains 0 in that point.\n",
    "\n",
    "$$\n",
    "\\text{prox}_{\\lambda,\\gamma}(z) = \\dots\n",
    "$$\n",
    "\n",
    "**Proximal Operator Formula:**\n",
    "\n",
    "For a given $\\gamma > 0$, the proximal operator for the LASSO regularization term $\\lambda \\|w\\|_1$ is defined as:\n",
    "$$\n",
    "\\text{prox}_{\\lambda, \\gamma}(z) = \\arg\\min_w \\left\\{ \\frac{1}{2\\gamma}\\|w - z\\|_2^2 + \\lambda \\|w\\|_1 \\right\\}\n",
    "$$\n",
    "\n",
    "**Proof that Subgradient Contains 0:**\n",
    "\n",
    "   The subgradient of the L1 norm $\\|w\\|_1$ at a point $w$ is given by:\n",
    "   $$\n",
    "   \\partial \\|w\\|_1 = \\left\\{ g \\in \\mathbb{R}^d : g_i \\in \\begin{cases}\n",
    "   \\{1\\} & \\text{if } w_i > 0\\\\\n",
    "   [-1, 1] & \\text{if } w_i = 0\\\\\n",
    "   \\{-1\\} & \\text{if } w_i < 0\n",
    "   \\end{cases} \\right\\}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5872-402c-45c2-b9f0-08aa2cbcb718",
   "metadata": {},
   "source": [
    "### Task 4: LASSO (OPTIONAL)\n",
    "\n",
    "Again, the LASSO is the following regularized optimization problem:\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\displaystyle\\min_{w\\in\\mathbb{R}^d} & \\frac{1}{n} \\left\\|{Xw-y}\\right\\|_2^2 + \\lambda \\|{w}\\|_1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "It is a regression method that provides sparse solutions/regressors. It is used when the data is very high-dimensional and only a few data points are given. This is usually the case when working with genome data. Here, one tries to predict which genes are responsible for a disease. Usually, we have only a few hundred data points (patients, the rows of the data matrix $X$) and a few thousand genes (the columns of the data matrix $X$). The label vector $y$ contains the label either $0$ for not having the disease and $1$ for having the disease. Without the regularization term infinitely many solutions exist. However, one usually assumes that only a few genes/features are responsible for causing the disease. Hence, we can use a $\\|{w}\\|_1$ regularizer to enforce sparse solutions.\n",
    "\n",
    "Implement a proximal gradient descent algorithm for solving the LASSO problem. Run it on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e1b6-a8e4-42f8-80c2-2661b099f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "lam = .5\n",
    "\n",
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X) + lam * np.linalg.norm(w, 1)\n",
    "\n",
    "def g(w):\n",
    "    return 2 * X.T @ (X @ w - y) / len(X) + lam * np.sign(w)\n",
    "\n",
    "def proximal_operator(w):\n",
    "    ...\n",
    "\n",
    "X, y = make_regression(n_samples=30, n_features=100, n_informative=2, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032b076-6e98-4b85-9f57-5cc830c7b41d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
