{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc565ac7-a7cb-43aa-8e7b-7cd17901667f",
   "metadata": {},
   "source": [
    "# Exercise sheet 7\n",
    "\n",
    "**Please turn in your exercises by January 9th.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e871567-aaa4-4bc9-809f-92536d8a061c",
   "metadata": {},
   "source": [
    "## Task 1: Covergence\n",
    "\n",
    "Provide a good intuition why the convergence rate of the subgradient method is $O(1/\\varepsilon^2)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55443425-346d-44f0-a68d-0c11611c5dc3",
   "metadata": {},
   "source": [
    "## Task 2: Gradient is sub-gradient\n",
    "\n",
    "Prove the following lemma.\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "If $f: \\mathbb{R}^n\\rightarrow \\mathbb{R}$ is convex and differentiable, then $\\forall x, \\partial f(x) =\\{\\nabla f(x)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30880ba4-d7e9-4d49-b6d6-1d27e8001b70",
   "metadata": {},
   "source": [
    "## Task 3: Convexity\n",
    "\n",
    "Prove the following lemma.\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "A function $f: D \\rightarrow \\mathbb{R}$, $D \\subseteq \\mathbb{R}^d$ is convex if and only if $D$ is convex and $\\forall x \\in D$, $\\partial f(x) \\neq \\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3884cb62-2014-43c0-8050-4c22cd19f75d",
   "metadata": {},
   "source": [
    "## Task 4: Smoothing\n",
    "\n",
    "You can write many convex, not necessarily differentiable function as\n",
    "$$\n",
    "f(x) = \\max_{i} f_i(x), \\quad 1\\leq i\\leq k\n",
    "$$\n",
    "for $k$ convex, differentiable functions $f_i$. One can \"smooth\" function $f$ by replacing $\\max$ with a realsoftmax/logsumexp, i.e.,\n",
    "$$\n",
    "g(x) = \\frac{1}{M}\\log\\left(\\sum_{i=1}^n \\exp(M\\cdot f_i(x))\\right)\n",
    "$$\n",
    "for a fixed parameter $M > 0$. The parameter $M$ controls how close $g$ is to $f$, i.e., how \"smooth\" the approximation $g$ is.\n",
    "\n",
    "Such a smoothing is a general way of turning any non-differentiable convex function into a differentiable function. If one does this, one can run Nesterov's gradient method on the resulting smooth function and it will take $O(1/\\varepsilon)$ many iterations to reach an absolute error of $\\varepsilon$ even on functions that are not necessarily strongly convex. Doesn't such an approach violate the lower bound of $O(1/\\varepsilon^2)$ for non-differentiable convex functions that are not necessarily strongly convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31b735-7a43-48e7-895f-6ac3467931e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ab7ff-2090-4470-9bf5-6eff31c862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526ec1b-fb2c-4e29-8c47-a07b6609fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_map(f, xb=(-1,1), yb=(-1,1), ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    (nx, ny) = (45, 45)\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    X = np.block([ [xv.reshape(1, -1)], [yv.reshape(1, -1)] ]).T\n",
    "    zv = np.fromiter((f(x) for x in X), dtype=np.double)\n",
    "    zv = zv.reshape(nx,ny)\n",
    "    ax.contour(xv, yv, zv, 15)\n",
    "\n",
    "def surface_plot(f, xb=(-1,1), yb=(-1,1)):\n",
    "    (nx, ny) = (45, 45)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    X = np.block([ [xv.reshape(1, -1)], [yv.reshape(1, -1)] ]).T\n",
    "    zv = np.fromiter((f(x) for x in X), dtype=np.double)\n",
    "    zv = zv.reshape(nx,ny)\n",
    "    ax.plot_surface(xv, yv, zv, cmap=cm.coolwarm)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3173a-453d-48fa-b0c0-8fddb861481c",
   "metadata": {},
   "source": [
    "## Task 5: Smoothing plots\n",
    "\n",
    "Using the idea from **Task 4**, plot the \"smooth\" approximation of the following functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db7ebc-8bcd-42f5-9201-5dafc603e962",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "Plot an approximation from Task 4 for the function\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(x, 0)\n",
    "$$\n",
    "for different values of $M$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6a515-7830-4816-8a43-c216f8b4294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.logspace(-2, 3, 5)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3db80-4175-42ae-bac9-0f7e7ec4baaf",
   "metadata": {},
   "source": [
    "### Absolute value\n",
    "\n",
    "Plot an approximation from Task 4 for the function\n",
    "$$\n",
    "\\text{abs}(x) = |x|\n",
    "$$\n",
    "for different values of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03064853-958c-4b08-bd7b-40e90fc93b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.logspace(-2, 3, 5)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e5872-402c-45c2-b9f0-08aa2cbcb718",
   "metadata": {},
   "source": [
    "## Task 6: Sub-gradient method\n",
    "\n",
    "Implement the sub-gradient method.\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the subgradient of `f`.\n",
    "\n",
    "Function `subgradient_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826621a3-fb82-4d1c-949f-509d2a7abc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_path(x0, f, g, max_iter=100):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91490c83-aeee-4192-84b5-9e2a92c0f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example\n",
    "f = lambda x: abs(x[0]) + abs(8*x[1])\n",
    "g = lambda x: np.sign(x) * [1, 8]\n",
    "\n",
    "x0 = np.array([-.5, .5])\n",
    "xs = subgradient_path(x0, f, g)\n",
    "xs = np.array(xs)\n",
    "\n",
    "contour_map(f)\n",
    "plt.plot(xs[:,0], xs[:,1], '-k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d4fb4-bc68-4edd-b877-13eefe7f52c9",
   "metadata": {},
   "source": [
    "### Robust regresssion\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\left\\|Xw - y\\right\\|_1\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a3354-d156-45e2-b09e-507ccbe5e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y, 1) / len(X)\n",
    "\n",
    "def g(w):\n",
    "    return ...\n",
    "    \n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aa632-0b89-449f-a81b-5d38eec3e2be",
   "metadata": {},
   "source": [
    "### Linear regression with $\\mathcal{l}_1$ regularization\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\left\\|Xw - y\\right\\|_2^2 + \\|w\\|_1,\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e1b6-a8e4-42f8-80c2-2661b099f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X) + np.linalg.norm(w, 1)\n",
    "\n",
    "def g(w):\n",
    "    return 2 * X.T @ (X @ w - y) / len(X) + ...\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104abd0-5728-4b25-ada7-3c574379f339",
   "metadata": {},
   "source": [
    "### Logistic regression with $\\mathcal{l}_1$ regularization\n",
    "\n",
    "Finish the gradient of the function\n",
    "$$\n",
    "f(w) = \\frac{1}{n}\\sum_{i=1}^n \\log(1 + \\exp(-y_i \\cdot x_i^\\top w)) + \\|w\\|_1\n",
    "$$\n",
    "run the sub-gradient method on it, and plot the function over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacc5fb-e053-4704-9634-ffe60ee42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(w):\n",
    "    return np.log(1. + np.exp(-y * X.dot(w))).mean() + np.linalg.norm(w, 1)\n",
    "\n",
    "def g(w):\n",
    "    sig = np.exp(-y * X.dot(w))\n",
    "    return - X.T.dot(sig * y / (sig + 1.)) / X.shape[0] + ...\n",
    "\n",
    "\n",
    "X, y = make_classification(1000, 80, n_informative=40,\n",
    "#                               n_redundant=0,\n",
    "                               n_clusters_per_class=2, flip_y=0.1, random_state=0)\n",
    "\n",
    "x0 = np.zeros(80)\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
