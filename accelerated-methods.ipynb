{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc565ac7-a7cb-43aa-8e7b-7cd17901667f",
   "metadata": {},
   "source": [
    "# Exercise sheet 4\n",
    "\n",
    "**Please turin in your exercises by November 28.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e871567-aaa4-4bc9-809f-92536d8a061c",
   "metadata": {},
   "source": [
    "## Task 1: L-smoothness\n",
    "\n",
    "In the lecture we saw the importance of $L$-smoothness of a function and its relation to the step size. Determining the $L$ constant can be really hard and time consuming. Prove the following upper bound on the largest signular value of the matrix.\n",
    "\n",
    "**Lemma**\n",
    "For all matrices $A \\in \\mathbb{R}^{n\\times m}$\n",
    "$$\n",
    "\\|A\\|_2 \\leq \\|A\\|_F\n",
    "$$\n",
    "where $\\|A\\|_2$ is the spectral norm and $\\|A\\|_F$ is the Frobenius norm of matrix $A$.\n",
    "\n",
    "**Proof**\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "Using the lemma above, approximate the constant $L$ of the function\n",
    "$$\n",
    "f(x) = \\|Ax - b\\|_2^2\n",
    "$$\n",
    "for the given matrices $A$ and vectors $b$ below. What is the true $L$ constant of those matrices?\n",
    "\n",
    "How much operations, in big-O notation, is needed to compute $\\|A\\|_2$ and how much to compute $\\|A\\|_F$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26721e67-5c66-499d-a4f1-87ca0f92d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "def f(x):\n",
    "    return np.linalg.norm(A@x - b)**2\n",
    "\n",
    "A = np.array([\n",
    "    [ 1,  2,  0],\n",
    "    [ 2,  2, -1],\n",
    "    [ 2,  1,  1],\n",
    "    [10,  0, -1],\n",
    "    [ 0,  0, 10],\n",
    "    [-1, -2, -1],\n",
    "    [-3, -2, -1]\n",
    "])\n",
    "b = np.array([[1, 1, 1, 0, 0, 0, 0]])\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "A = rng.normal(1., 2., size=(10000, 4000))\n",
    "b = np.ones(10000)\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55443425-346d-44f0-a68d-0c11611c5dc3",
   "metadata": {},
   "source": [
    "## Task 2: Running time\n",
    "\n",
    "Gradient descent has a convergence rate of $O((1 - \\frac{\\mu}{L})^T)$ for strongly convex functions. Compare this to the convergence rate of the optimal method of $O((1 - \\sqrt{\\frac{\\mu}{L}})^T)$. How many more iterations does gradient descent need as compared to the optimal method to achieve an approximate error of $\\varepsilon$ for a function with condition number $Q_f := \\frac{L}{\\mu}$?\n",
    "\n",
    "Plot the convergence rate over time for $Q_f = 10^4$ (assume big-O constanst are 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9378fb8-36ed-460b-be89-84789c94ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31b735-7a43-48e7-895f-6ac3467931e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ab7ff-2090-4470-9bf5-6eff31c862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526ec1b-fb2c-4e29-8c47-a07b6609fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_map(f, xb=(-1,1), yb=(-1,1), ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    (nx, ny) = (45, 45)\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    X = np.block([ [xv.reshape(1, -1)], [yv.reshape(1, -1)] ]).T\n",
    "    zv = np.fromiter((f(x) for x in X), dtype=np.double)\n",
    "    zv = zv.reshape(nx,ny)\n",
    "    ax.contour(xv, yv, zv, 15)\n",
    "\n",
    "def surface_plot(f, xb=(-1,1), yb=(-1,1)):\n",
    "    (nx, ny) = (45, 45)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = np.linspace(*xb, nx)\n",
    "    y = np.linspace(*yb, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    X = np.block([ [xv.reshape(1, -1)], [yv.reshape(1, -1)] ]).T\n",
    "    zv = np.fromiter((f(x) for x in X), dtype=np.double)\n",
    "    zv = zv.reshape(nx,ny)\n",
    "    ax.plot_surface(xv, yv, zv, cmap=cm.coolwarm)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4139b34-74b0-4c7b-8094-9fe78ede4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(x, d, f, g, alpha=0.3, beta=0.8):\n",
    "    step_size = 1.\n",
    "    while f(x + step_size * d) > f(x) + alpha * step_size * g(x).dot(d):\n",
    "        step_size *= beta\n",
    "    return step_size\n",
    "\n",
    "def gradient_descent_path(x0, f, g, max_iter=100):\n",
    "    xs = [x0]\n",
    "    for _ in range(max_iter):\n",
    "        step = backtracking_line_search(x0, -g(x0), f, g)\n",
    "        x0 = x0 - step * g(x0)\n",
    "        xs.append(x0)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816f59c-9dba-4f72-a144-271e5152046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXA = np.array([[30., 15],[-20, 25]])/20\n",
    "\n",
    "EXAMPLES = [\n",
    "    (lambda x: x.T@EXA@x + 1, lambda x: (EXA+EXA.T)@x, 0.5 * np.ones(2), (-1.,1.), (-1.,1.)),\n",
    "    (lambda x: (x[0]**2 + 30 * x[1]**2 + 4 * x[0]), lambda x: np.array([2 * x[0] + 4, 60 * x[1]]), np.array([2.,3.]), (-2.5,2.5), (-1.5,3.5)),\n",
    "    (lambda x: np.linalg.norm(np.sin(x*3))**2, lambda x: 6 * np.sin(x*3) * np.cos(x*3), np.array([.3,.5]), (-1.5, 1.), (-1.,1.))\n",
    "]\n",
    "\n",
    "def run_examples_heavy_ball():\n",
    "    for (f, g, x0, xb, yb) in EXAMPLES:\n",
    "        xs2 = heavy_ball_path(x0, f, g)\n",
    "        xs2 = np.array(xs2)\n",
    "        \n",
    "        xs = gradient_descent_path(x0, f, g)\n",
    "        xs = np.array(xs)\n",
    "\n",
    "        contour_map(f, xb=xb, yb=yb)\n",
    "        plt.plot(xs2[:,0], xs[:,1], '.--k', label='heavy ball')\n",
    "        plt.plot(xs[:,0], xs[:,1], '.--', color='gray', alpha=.5, label='gradient descent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def run_examples_nag():\n",
    "    for (f, g, x0, xb, yb) in EXAMPLES:\n",
    "        xs2 = nag_path(x0, f, g)\n",
    "        xs2 = np.array(xs2)\n",
    "        \n",
    "        xs = gradient_descent_path(x0, f, g)\n",
    "        xs = np.array(xs)\n",
    "\n",
    "        contour_map(f, xb=xb, yb=yb)\n",
    "        plt.plot(xs2[:,0], xs[:,1], '.--k', label='nesterov')\n",
    "        plt.plot(xs[:,0], xs[:,1], '.--', color='gray', alpha=.5, label='gradient descent')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3173a-453d-48fa-b0c0-8fddb861481c",
   "metadata": {},
   "source": [
    "## Task 3: Polyak's heavy ball method\n",
    "\n",
    "Implement Polyak's heavy ball method and the coresponding line search. **Standard backtracking line search is not usable here.**\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the gradient of `f`.\n",
    "\n",
    "Function `heavy_ball_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6a515-7830-4816-8a43-c216f8b4294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_ball_path(x0, f, g, max_iter=100):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87802424-9573-4cea-9db8-1a3317ab5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to get plots\n",
    "run_examples_heavy_ball()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d885a3e-ac36-4831-89e3-8a1502c0e1a8",
   "metadata": {},
   "source": [
    "## Task 4: Nesterov accelerated gradient method and backtracking line search\n",
    "\n",
    "Implement Nesterov's accelerated gradient method and the coresponding line search. **Standard backtracking line search is not usable here.**\n",
    "\n",
    "* `x0` is the initial point.\n",
    "* `f` is the function you are trying to minimize.\n",
    "* `g` is the gradient of `f`.\n",
    "\n",
    "Function `nag_path` should return a list of vectors on the path to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18859c3b-7091-4d83-8fcc-4aeb6e65ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nag_path(x0, f, g, max_iter=100):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c907b5-9230-4e93-bfc3-119f095365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_examples_nag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a9ce0-1a09-45e0-953e-6836a52c8474",
   "metadata": {},
   "source": [
    "## Task 5: Error plots\n",
    "\n",
    "Compare and plot the error over time for the three methods on the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b032adb-e1c2-4d0f-a258-d0ba56ee3e28",
   "metadata": {},
   "source": [
    "### Simple quadratic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d75b4-4dca-41eb-b2ac-55144aa0126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: (x[0]**2 + 30 * x[1]**2 + 4 * x[0])\n",
    "g = lambda x: np.array([2 * x[0] + 4, 60 * x[1]])\n",
    "x0 = np.array([2.,3.])\n",
    "\n",
    "x_star = np.array([-2.,0.])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aa632-0b89-449f-a81b-5d38eec3e2be",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e1b6-a8e4-42f8-80c2-2661b099f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "def f(w):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 / len(X)\n",
    "\n",
    "def g(w):\n",
    "    return 2 * X.T @ (X @ w - y) / len(X)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=100, n_informative=40, random_state=0)\n",
    "x0 = np.zeros(100)\n",
    "\n",
    "x_star = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104abd0-5728-4b25-ada7-3c574379f339",
   "metadata": {},
   "source": [
    "### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacc5fb-e053-4704-9634-ffe60ee42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def f(w):\n",
    "    return np.log(1. + np.exp(-y * X.dot(w))).mean() + np.linalg.norm(w)**2\n",
    "\n",
    "def g(w):\n",
    "    sig = np.exp(-y * X.dot(w))\n",
    "    return 2*w - X.T.dot(sig * y / (sig + 1.)) / X.shape[0]\n",
    "\n",
    "X, y = make_classification(1000, 80, n_informative=40,\n",
    "#                               n_redundant=0,\n",
    "                               n_clusters_per_class=2, flip_y=0.1, random_state=0)\n",
    "\n",
    "x0 = np.zeros(80)\n",
    "x_star = minimize(f, x0, jac=g).x\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
